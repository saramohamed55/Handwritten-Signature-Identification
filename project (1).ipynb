{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as logistic\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(str):\n",
    "    data = []\n",
    "    for per in os.listdir('C:/Users/Hp/Documents/fourth/first/VISOIN/project/Dataset'):\n",
    "        count = 0\n",
    "        for img_path in glob.glob('C:/Users/Hp/Documents/fourth/first/VISOIN/project/Dataset/'+per+'/'+ str+ '/*.png'):\n",
    "            \n",
    "            count += 1\n",
    "            if (count > 40 and str == 'Train') or (count > 8 and str == 'Test'):\n",
    "                break\n",
    "            \n",
    "            img_name = img_path.split('\\\\')[-1]\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "\n",
    "            data.append([img, per, img_name])\n",
    "            \n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift(data):\n",
    "    sift = cv2.SIFT_create()\n",
    "    desc = []\n",
    "\n",
    "    for row in data:\n",
    "        kp, des = sift.detectAndCompute(row[0], None)\n",
    "        desc.append(des)\n",
    "    \n",
    "    desc_stack = np.array(desc[0])\n",
    "    for remaining in desc[1:]:\n",
    "        desc_stack = np.vstack((desc_stack, remaining))\n",
    "\n",
    "    return desc_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(desc_stack, n_cluster):\n",
    "\n",
    "    kmeans_model = KMeans(n_clusters=n_cluster)\n",
    "    cluster_model = kmeans_model.fit(desc_stack)\n",
    "\n",
    "    return cluster_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_exctract(cluster_model, data, desc_stack, n_cluster):\n",
    "\n",
    "    clusters = cluster_model.predict(desc_stack)\n",
    "\n",
    "    histograms = np.array([np.zeros(n_cluster) for i in range(len(data))])\n",
    "\n",
    "    count = 0\n",
    "    final_data = []\n",
    "    for i in range(len(data)):\n",
    "        l = len(data[i])\n",
    "        for j in range(l):\n",
    "            index = clusters[count + j]\n",
    "            histograms[i][index] += 1\n",
    "        count += l\n",
    "    \n",
    "    std_histograms = StandardScaler().fit_transform(histograms)\n",
    "\n",
    "    final_data = []\n",
    "\n",
    "    for i in range(len(std_histograms)):\n",
    "        row = []\n",
    "        for j in range(len(std_histograms[i])):\n",
    "            row.append(std_histograms[i, j])\n",
    "\n",
    "        row.append(data[i][1])\n",
    "        row.append(data[i][-1])\n",
    "        \n",
    "        final_data.append(row)\n",
    "    \n",
    "    columns = []\n",
    "\n",
    "    for i in range(n_cluster):\n",
    "        columns.append('feature' + str(i))\n",
    "    \n",
    "    columns.append('person_name')\n",
    "    columns.append('image_name')\n",
    "\n",
    "    final_df = pd.DataFrame(final_data, columns=columns)\n",
    "\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labels(features, t, per):\n",
    "\n",
    "    csv_path = 'C:/Users/Hp/Documents/fourth/first/VISOIN/project/Dataset/'+per+'/'+ t + '/' + per + '_SigVerification' + t +'Labels.csv'\n",
    "\n",
    "    per_csv = pd.read_csv(csv_path)\n",
    "\n",
    "    per_groups = features.groupby(['person_name'])\n",
    "\n",
    "    per_data = per_groups.get_group(per)\n",
    "    \n",
    "    labeled_data = per_data.merge(per_csv, how='inner', on='image_name')\n",
    "\n",
    "    enc = OrdinalEncoder()\n",
    "    enc_labels = enc.fit_transform(np.array(labeled_data['label']).reshape(-1, 1))\n",
    "    labeled_data['enc_label'] = pd.DataFrame(enc_labels)\n",
    "\n",
    "    labeled_data.drop(['image_name', 'person_name', 'label'], axis=1, inplace=True)\n",
    "    \n",
    "    return labeled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_cluster):\n",
    "    models=[]\n",
    "    for per in os.listdir('C:/Users/Hp/Documents/fourth/first/VISOIN/project/Dataset'):\n",
    "\n",
    "        files = read_files(\"Train\")\n",
    "\n",
    "        desc_stack = sift(files)\n",
    "\n",
    "        cluster_model = cluster(desc_stack, n_cluster)\n",
    "        \n",
    "        Features = feature_exctract(cluster_model, files, desc_stack, n_cluster)\n",
    "\n",
    "        Data = merge_labels(Features, \"Train\", per)\n",
    "\n",
    "        X_train, Y_train = Data.iloc[:, :-1], Data[\"enc_label\"]\n",
    "\n",
    "        \n",
    "\n",
    "       # classifier = logistic()\n",
    "        classifier=GaussianNB()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        \n",
    "        models.append(classifier)\n",
    "\n",
    "    return models, cluster_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(cluster_model, models, n_cluster):\n",
    "    count=0\n",
    "\n",
    "    for per in os.listdir('C:/Users/Hp/Documents/fourth/first/VISOIN/project/Dataset'):\n",
    "        \n",
    "        files = read_files(\"Test\")\n",
    "\n",
    "        desc_stack = sift(files)\n",
    "\n",
    "        Features = feature_exctract(cluster_model, files, desc_stack, n_cluster)\n",
    "\n",
    "        Data = merge_labels(Features, \"Test\", per)\n",
    "\n",
    "        X_test, Y_test = Data.iloc[:, :-1], Data[\"enc_label\"]\n",
    "\n",
    "        y_predict = models[count].predict(X_test)\n",
    "    \n",
    "        print(\"Accuracy score %.3f\" %metrics.accuracy_score(Y_test, y_predict))\n",
    "       \n",
    "        count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.429\n",
      "Accuracy score 0.375\n",
      "Accuracy score 0.125\n",
      "Accuracy score 0.500\n",
      "Accuracy score 0.750\n"
     ]
    }
   ],
   "source": [
    "models, cluster_model = train_model(60)\n",
    "test_model(cluster_model, models, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4df2cc69eb3fd56a8a9780b07025dcfa15673fb6f5e64f079d5772d6ef5f08ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
